# -*- coding: utf-8 -*-
"""UNET_chatGPT_V3.3.2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krymnzYHr8CGWPj17a1LwnBE6FbA7sVh

"""


import os
os.environ["XLA_FLAGS"] = "--xla_gpu_cuda_data_dir=/home/trung/cuda-12.3/nvvm/libdevice"


import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from sklearn.utils import shuffle

from tqdm import tqdm


import tifffile as tiff
import csv

import rasterio


import matplotlib.pyplot as plt

# Set GPU memory growth to avoid RAM overload
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)



def get_npy_filepaths(img_dir, mask_dir):
    img_files = sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith('.npy')])
    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.npy')])
    return shuffle(img_files, mask_files, random_state=42)

def parse_image_mask(img_path, mask_path):
    def _load_npy(img, mask):
        img = img.decode("utf-8")
        mask = mask.decode("utf-8")

        img_arr = np.load(img).astype('float32')
        mask_arr = np.load(mask).astype('int32')
       

        # Handle mask shape
        if mask_arr.ndim == 3:
            mask_arr = mask_arr[:, :, 0]  # squeeze out extra dim if needed


        valid_mask = (mask_arr >= 0) & (mask_arr < NUM_CLASSES)

        clipped_mask = np.where(valid_mask, mask_arr, 0)  # temporarily set 255 to 0
        mask_arr = to_categorical(clipped_mask, num_classes=NUM_CLASSES).astype('float32')


        # Zero out invalid pixels (255) in the one-hot encoded mask
        mask_arr *= np.expand_dims(valid_mask, axis=-1)

        return img_arr, mask_arr

    img, mask = tf.numpy_function(_load_npy, [img_path, mask_path], [tf.float32, tf.float32])
    img.set_shape((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))
    mask.set_shape((IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES))

    return img, mask

def create_dataset(img_paths, mask_paths, batch_size=16, shuffle=True):
    dataset = tf.data.Dataset.from_tensor_slices((img_paths, mask_paths))
    dataset = dataset.map(parse_image_mask, num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        dataset = dataset.shuffle(buffer_size=1000)
    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset

def unetpp_model_tf(input_shape, num_classes, l2_factor=1e-4):
    inputs = tf.keras.layers.Input(shape=input_shape)

    def conv_block(x, filters, dropout_rate=0.3):
        x = tf.keras.layers.Conv2D(filters, 3, padding='same',
                                   kernel_regularizer=tf.keras.regularizers.l2(l2_factor))(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Activation('relu')(x)
        x = tf.keras.layers.Conv2D(filters, 3, padding='same',
                                   kernel_regularizer=tf.keras.regularizers.l2(l2_factor))(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Activation('relu')(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        return x

    def upsample_concat(x, skip):
        x = tf.keras.layers.Conv2DTranspose(tf.keras.backend.int_shape(skip)[-1], 2, strides=2, padding='same')(x)
        return tf.keras.layers.concatenate([x, skip])

    # Encoder path (8 levels)
    x00 = conv_block(inputs, 16, 0.1)
    p0 = tf.keras.layers.MaxPooling2D()(x00)

    x10 = conv_block(p0, 32, 0.12)
    p1 = tf.keras.layers.MaxPooling2D()(x10)

    x20 = conv_block(p1, 64, 0.15)
    p2 = tf.keras.layers.MaxPooling2D()(x20)

    x30 = conv_block(p2, 128, 0.18)
    p3 = tf.keras.layers.MaxPooling2D()(x30)

    x40 = conv_block(p3, 256, 0.20)
    p4 = tf.keras.layers.MaxPooling2D()(x40)

    x50 = conv_block(p4, 512, 0.25)
    p5 = tf.keras.layers.MaxPooling2D()(x50)

    x60 = conv_block(p5, 1024, 0.30)
    p6 = tf.keras.layers.MaxPooling2D()(x60)

    x70 = conv_block(p6, 2048, 0.35)  # bottleneck

    # Decoder with nested connections (partial, expand as needed)
    x61 = conv_block(upsample_concat(x70, x60), 1024, 0.3)
    x51 = conv_block(upsample_concat(x61, x50), 512, 0.25)
    x41 = conv_block(upsample_concat(x51, x40), 256, 0.20)
    x31 = conv_block(upsample_concat(x41, x30), 128, 0.18)
    x21 = conv_block(upsample_concat(x31, x20), 64, 0.15)
    x11 = conv_block(upsample_concat(x21, x10), 32, 0.12)
    x01 = conv_block(upsample_concat(x11, x00), 16, 0.1)

    outputs = tf.keras.layers.Conv2D(num_classes, 1, activation='softmax')(x01)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model

def classify_full_image(image_path, model, patch_size=128, stride=32):
    img = tiff.imread(image_path)  # shape: (H, W, Bands)i
    H, W, C = img.shape
    prob_map = np.zeros((H, W, model.output_shape[-1]), dtype=np.float32)
    count_map = np.zeros((H, W, 1), dtype=np.float32)

    # Calculate total number of steps for progress bar
    total_steps = ((H - 1) // stride + 1) * ((W - 1) // stride + 1)
    pbar = tqdm(total=total_steps, desc="Predicting patches")


    for i in range(0,H, stride):
        for j in range(0, W , stride):
            i_start = min(i, H - patch_size)
            j_start = min(j, W - patch_size)
            
            patch = img[i_start:i_start+patch_size, j_start:j_start+patch_size, :]
            patch = patch.astype('float32')
            patch = np.nan_to_num(patch, nan=0.0, posinf=0.0, neginf=0.0)
            
            
            pred = model.predict(patch[np.newaxis, ...],verbose=0)[0]  # shape: (patch_size, patch_size, num_classes)

            prob_map[i_start:i_start+patch_size, j_start:j_start+patch_size, :] += pred
            count_map[i_start:i_start+patch_size, j_start:j_start+patch_size, :] += 1
            pbar.update(1)
    pbar.close()
    # Avoid division by zero
    count_map[count_map == 0] = 1
    prob_map /= count_map
    prediction_map = np.argmax(prob_map, axis=-1).astype(np.uint8)
    return prediction_map

def save_classified_geotiff(classified_array, reference_path, output_path):
    # Use a reference Sentinel-1 image for geo info
    with rasterio.open(reference_path) as src:
        profile = src.profile
        profile.update({
            'count': 1,
            'dtype': 'uint8',  # change based on your class dtype
            'compress': 'lzw'
        })

        with rasterio.open(output_path, 'w', **profile) as dst:
            dst.write(classified_array.astype('byte'), 1)

def track_class_counts(dataset, num_classes):
    class_counts = np.zeros(num_classes, dtype=np.int64)

    def generator():
        for x_batch, y_batch in dataset:
            y_np = y_batch.numpy()
            batch_counts = np.sum(y_np, axis=(0, 1, 2))  # One-hot format
            class_counts[:] += batch_counts.astype(np.int64)
            yield x_batch, y_batch

    for sample_x, sample_y in dataset.take(1):
        x_spec = tf.TensorSpec(sample_x.shape, sample_x.dtype)
        y_spec = tf.TensorSpec(sample_y.shape, sample_y.dtype)
        break

    tracked_dataset = tf.data.Dataset.from_generator(
        generator,
        output_signature=(x_spec, y_spec)
    )

    return tracked_dataset, class_counts

class DynamicWeightsCallback(tf.keras.callbacks.Callback):
    def __init__(self, class_counts, smoothing=1.0):
        super().__init__()
        self.class_counts = class_counts
        self.smoothing = smoothing
        self.weights = self._compute_weights()

    def _compute_weights(self):
        total = np.sum(self.class_counts) + 1e-6
        freqs = self.class_counts / total
        weights = 1.0 / (freqs + self.smoothing)
        return weights / np.sum(weights)

    def get_weights(self):
        return self.weights.astype(np.float32)

    def on_epoch_end(self, epoch, logs=None):
        self.weights = self._compute_weights()

from tensorflow.keras.saving import register_keras_serializable
@register_keras_serializable()
class CombinedLoss(tf.keras.losses.Loss):
    def __init__(self, get_weights, ignore_class=255, alpha=tf.Variable(1.0), beta=tf.Variable(1.0), name="combined_loss", **kwargs):
        super().__init__(name=name, **kwargs)
        self.get_weights = get_weights
        self.alpha = alpha
        self.beta = beta
        self.ignore_class = ignore_class

    def call(self, y_true, y_pred):
        weights = tf.constant(self.get_weights(), dtype=tf.float32)
        
        # Convert one-hot to class indices to create mask
        y_true_cls = tf.argmax(y_true, axis=-1)
        mask = tf.not_equal(y_true_cls, self.ignore_class)

        # Apply mask
        mask = tf.cast(mask, tf.float32)

        # Compute weighted CE
        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)  # shape: (batch, H, W)
        pixel_weights = tf.reduce_sum(y_true * weights, axis=-1)       # shape: (batch, H, W)
        weighted_ce = ce * pixel_weights * mask
        loss_ce = tf.math.divide_no_nan(tf.reduce_sum(weighted_ce), tf.reduce_sum(mask))

        # Compute masked Dice loss
        intersection = tf.reduce_sum(y_true * y_pred * tf.expand_dims(mask, -1))
        denominator = tf.reduce_sum((y_true + y_pred) * tf.expand_dims(mask, -1))
        dice = (2. * intersection + 1e-7) / (denominator + 1e-7)
        loss_dice = 1. - dice

        return self.alpha * loss_ce + self.beta * loss_dice

    def get_config(self):
        config = super().get_config()
        config.update({
            #"alpha": self.alpha,
            #"beta": self.beta,
            "alpha": float(self.alpha.numpy()),
            "beta": float(self.beta.numpy()),    
            "ignore_class": self.ignore_class
        })
        return config

    @classmethod
    def from_config(cls, config, get_weights=None):
        if get_weights is None:
            raise ValueError("You must provide 'get_weights' when loading this loss, which cannot be deserialized automatically.")
        return cls(get_weights=get_weights, **config)

class AdaptiveLossCallback(tf.keras.callbacks.Callback):
    def __init__(
        self,
        loss,
        validation_data,
        num_classes,
        ignore_classes=[255],
        csv_path="adaptive_loss_log.csv",
        patience=3,
        slope_scale=1.0,
        min_weight=0.1,
        max_weight=0.9,
        cooldown=2,
    ):
        super().__init__()
        self.loss = loss
        self.validation_data = validation_data
        self.num_classes = num_classes
        self.ignore_classes = ignore_classes
        self.csv_path = csv_path
        self.patience = patience
        self.slope_scale = slope_scale
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.cooldown = cooldown
        self.cooldown_counter = 0
        self.wait = 0
        self.best_iou = 0.0
        self.val_ious = []
        self.history = {
            "epoch": [],
            "val_loss": [],
            "val_mean_iou": [],
            "alpha": [],
            "beta": [],
        }

        self.per_class_iou_header = [f"iou_class_{i}" for i in range(self.num_classes)]

        # Prepare CSV header
        if not os.path.exists(csv_path):
            with open(csv_path, mode="w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["epoch", "val_loss", "val_mean_iou", "alpha", "beta"] + self.per_class_iou_header)

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        val_iou = logs.get("val_mean_io_u")
        val_loss = logs.get("val_loss")

        if val_iou is None:
            print("val_mean_io_u not found in logs.")
            return

        alpha = float(self.loss.alpha.numpy())
        beta = float(self.loss.beta.numpy())

        # Compute per-class IoU
        per_class_ious = self._compute_per_class_iou()

        # Save logs to internal history
        self.val_ious.append(val_iou)
        self.history["epoch"].append(epoch)
        self.history["val_loss"].append(val_loss)
        self.history["val_mean_iou"].append(val_iou)
        self.history["alpha"].append(alpha)
        self.history["beta"].append(beta)

        # Save to CSV
        with open(self.csv_path, mode="a", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([epoch, val_loss, val_iou, alpha, beta] + per_class_ious)

        if len(self.val_ious) <= self.patience:
            return

        x = np.arange(-self.patience, 1)
        y = self.val_ious[-self.patience - 1:]
        slope = np.polyfit(x, y, 1)[0]

        if self.cooldown_counter > 0:
            self.cooldown_counter -= 1
            return

        if val_iou > self.best_iou:
            self.best_iou = val_iou
            self.wait = 0
            return
        else:
            self.wait += 1

        if self.wait >= self.patience:
            delta = slope * self.slope_scale
            delta = np.clip(delta, -0.1, 0.1)  # limit big jumps
            if abs(delta) < 0.05:
                delta = 0.01 * np.sign(delta)  # force minimum adjustment
                        
            
            current_alpha = alpha
            current_beta = beta

            alpha_a = np.clip(current_alpha * (1 + delta), self.min_weight, self.max_weight)
            beta_a = np.clip(current_beta * (1 - delta), self.min_weight, self.max_weight)

            alpha_b = np.clip(current_alpha * (1 - delta), self.min_weight, self.max_weight)
            beta_b = np.clip(current_beta * (1 + delta), self.min_weight, self.max_weight)

            print(f"\n[AdaptiveLoss] Epoch {epoch}: slope={slope:.4f}, trying both ↑α and ↓α")

            self.loss.alpha.assign(alpha_a)
            self.loss.beta.assign(beta_a)
            iou_a = self._evaluate_on_validation().get("val_mean_io_u", 0)

            self.loss.alpha.assign(alpha_b)
            self.loss.beta.assign(beta_b)
            iou_b = self._evaluate_on_validation().get("val_mean_io_u", 0)

            if iou_a >= iou_b:
                self.loss.alpha.assign(alpha_a)
                self.loss.beta.assign(beta_a)
                print(f"[AdaptiveLoss] ↑α chosen: α={alpha_a:.4f}, β={beta_a:.4f}, val_mIoU={iou_a:.4f}")
            else:
                self.loss.alpha.assign(alpha_b)
                self.loss.beta.assign(beta_b)
                print(f"[AdaptiveLoss] ↓α chosen: α={alpha_b:.4f}, β={beta_b:.4f}, val_mIoU={iou_b:.4f}")

            self.wait = 0
            self.cooldown_counter = self.cooldown

    def _evaluate_on_validation(self):
        logs = self.model.evaluate(self.validation_data, return_dict=True, verbose=0)
        return logs

    def _compute_per_class_iou(self):
        y_true_list = []
        y_pred_list = []

        for x_batch, y_true in self.validation_data:
            y_pred = self.model.predict(x_batch, verbose=0)
            y_true = tf.argmax(y_true, axis=-1)
            y_pred = tf.argmax(y_pred, axis=-1)

            y_true_list.append(tf.reshape(y_true, [-1]))
            y_pred_list.append(tf.reshape(y_pred, [-1]))

        y_true_flat = tf.concat(y_true_list, axis=0)
        y_pred_flat = tf.concat(y_pred_list, axis=0)

        per_class_ious = []

        for i in range(self.num_classes):
            if i in self.ignore_classes:
                per_class_ious.append(None)
                continue

            true_mask = tf.equal(y_true_flat, i)
            pred_mask = tf.equal(y_pred_flat, i)
            intersection = tf.reduce_sum(tf.cast(tf.logical_and(true_mask, pred_mask), tf.float32))
            union = tf.reduce_sum(tf.cast(tf.logical_or(true_mask, pred_mask), tf.float32))

            iou = (intersection / union).numpy() if union.numpy() > 0 else 0.0
            per_class_ious.append(round(iou, 4))

        return per_class_ious

    def on_train_end(self, logs=None):
        self._plot_metrics()

    def _plot_metrics(self):
        epochs = self.history["epoch"]

        plt.figure(figsize=(14, 10))

        plt.subplot(2, 2, 1)
        plt.plot(epochs, self.history["val_mean_iou"], marker="o", color="green")
        plt.title("Validation Mean IoU")
        plt.xlabel("Epoch")
        plt.ylabel("IoU")
        plt.grid(True)

        plt.subplot(2, 2, 2)
        plt.plot(epochs, self.history["val_loss"], marker="o", color="red")
        plt.title("Validation Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.grid(True)

        plt.subplot(2, 2, 3)
        plt.plot(epochs, self.history["alpha"], marker="o", color="blue")
        plt.title("Alpha Over Epochs")
        plt.xlabel("Epoch")
        plt.ylabel("Alpha")
        plt.grid(True)

        plt.subplot(2, 2, 4)
        plt.plot(epochs, self.history["beta"], marker="o", color="orange")
        plt.title("Beta Over Epochs")
        plt.xlabel("Epoch")
        plt.ylabel("Beta")
        plt.grid(True)

        plt.tight_layout()
        plt.savefig("adaptive_loss_plot.png", dpi=300)



# --> DECLARE THE INPUT IMAGE AND MASK IMAGE HERE

predictedImage_path = '/mnt/RAID100TB/work_RAID/work_Trung/AgrUNET/'  # Directory to save predicted masks
model_path = '/mnt/RAID100TB/work_RAID/work_Trung/AgrUNET/unetpp_model.keras'

PATCH_IMAGE_DIR = '/mnt/RAID100TB/work_RAID/work_Trung/AgrUNET/S12_23bands_Patch_aug'
PATCH_MASK_DIR = '/mnt/RAID100TB/work_RAID/work_Trung/AgrUNET/S12_23bands_MaskPatch_aug'
PATCH_IMAGE_TEST_DIR = '/mnt/RAID100TB/work_RAID/work_Trung/AgrUNET/S12_23bands_Patch_aug_val'
PATCH_MASK_TEST_DIR = '/mnt/RAID100TB/work_RAID/work_Trung/AgrUNET/S12_23bands_MaskPatch_aug_val'

# --> SET PARAMETERS OF MODEL HERE
#ATTENTION
# DO NOT CHANGE THE NAME OF THE VARIABLE, SINCE FUNCTIONS USE IT
# Change normalized method of classification function based on the method of normalized when trained data
IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 128, 128, 23  # Image dimensions and bands
NUM_CLASSES = 16  # Number of categories
IGNORE_CLASS = None  # or index to ignore
BATCH_SIZE = 32
EPOCHS = 100
L2_FACTOR = 1e-4
SEED = 42
alpha = 0.3 # adjust alpha and beta for combine diceloss (beta) and ce (alpha) (alpha * loss_ce + beta * loss_dice)
beta = 0.7

# --> SET PARAMETERS OF INPUT AND OUTPUT FOLDER PATH FOR IMAGE CLASSIFICATION HERE
input_folder = '/mnt/RAID100TB/work_RAID/work_Trung/AgrUNET/S12_23bands'
predictedImage_path = '/mnt/RAID100TB/work_RAID/work_Trung/AgrUNET/result_S12_23bands_unetpp'


train_imgs, train_masks = get_npy_filepaths(PATCH_IMAGE_DIR, PATCH_MASK_DIR)
test_imgs, test_masks = get_npy_filepaths(PATCH_IMAGE_TEST_DIR, PATCH_MASK_TEST_DIR)

# Create datasets
train_dataset = create_dataset(train_imgs, train_masks, BATCH_SIZE, shuffle=True)
val_dataset = create_dataset(test_imgs, test_masks, BATCH_SIZE, shuffle=False)


# Ajdust dynamic weight
train_dataset_tracked, class_counts = track_class_counts(train_dataset, num_classes=NUM_CLASSES)
dynamic_weights_cb = DynamicWeightsCallback(class_counts)

if os.path.exists(model_path):
    print("📂 Loading existing model without compiling...")
    model = tf.keras.models.load_model(
        model_path,
        #custom_objects={'PerClassIoU': PerClassIoU},
        compile=False  # do NOT compile yet
    )
    print("✅ Model loaded, now compiling with custom loss and metrics...")
    #loss = CombinedLoss(get_weights=dynamic_weights_cb.get_weights, alpha=alpha, beta=beta)
    loss = CombinedLoss(get_weights=dynamic_weights_cb.get_weights, alpha=tf.Variable(0.5), beta=tf.Variable(0.5))
    model.compile(
        optimizer='adam',
        loss=loss,
        metrics=[
            'accuracy',
            tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES),
            tf.keras.metrics.Precision(),
            tf.keras.metrics.Recall()
            #PerClassIoU(num_classes=NUM_CLASSES)
        ]
    )
else:
    print("🆕 No model found. Creating a new model...")
    model = unetpp_model_tf((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), NUM_CLASSES, L2_FACTOR)
    #loss = CombinedLoss(get_weights=dynamic_weights_cb.get_weights, alpha=alpha, beta=beta)
    loss = CombinedLoss(get_weights=dynamic_weights_cb.get_weights, alpha=tf.Variable(0.5), beta=tf.Variable(0.5))
    model.compile(
        optimizer='adam',
        loss=loss,
        metrics=[
            'accuracy',
            tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES),
            tf.keras.metrics.Precision(),
            tf.keras.metrics.Recall(),
            #PerClassIoU(num_classes=NUM_CLASSES)
        ]
    )


model.summary()

# Stop training when validation loss doesn't improve for 10 epochs
early_stop_cb = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True,
    verbose=1
)

# Save the best model based on validation loss
checkpoint_cb = ModelCheckpoint(
    filepath=model_path,
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=False,  # Set to True if you're saving only weights
    verbose=1
)

#Adjust alpha and beta of loss function
adaptive_callback = AdaptiveLossCallback(
    loss=loss,
    validation_data=val_dataset,         # Your tf.data.Dataset for validation
    num_classes=NUM_CLASSES,
    ignore_classes=[255],
    csv_path=predictedImage_path + '.csv',
    patience=3,
    slope_scale=0.5,
    min_weight=0.1,
    max_weight=0.9,
    cooldown=2
)


#Learning rate
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-6
)


history = model.fit(train_dataset,
                    validation_data=val_dataset,
                    epochs=EPOCHS,
                    #callbacks=[checkpoint, early_stop]
                    callbacks=[
                           dynamic_weights_cb,  # if you're using dynamic weights
                           early_stop_cb,
                           checkpoint_cb,
                           reduce_lr,
                           adaptive_callback,
                               ]
                    )


print("Training model completed.")
model.save(model_path)
print(f"Model saved to {model_path}")

# Create the output directory if it doesn't exist
os.makedirs(predictedImage_path, exist_ok=True)

# Iterate through all image files in the input folder
for filename in os.listdir(input_folder):
    if filename.lower().endswith(('.tif', '.tiff')):  # You can add other image extensions if needed
        input_path = os.path.join(input_folder, filename)

        print(f'Processing: {filename}')
        # Classify the image
        classified_map = classify_full_image(input_path, model)
        # Save the result
        outname = filename[:-4] + '_predicted.tif'
        out_path = os.path.join(predictedImage_path, outname)
        save_classified_geotiff(classified_map, reference_path=input_path, output_path=out_path)
        print(f'Saved: {out_path}')
